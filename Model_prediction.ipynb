{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Latitude  Longitude  Elevation       Date   Time\n",
      "0  11.67362   76.26272      879.0  01-Jan-19  06:00\n",
      "1  11.67396   76.26302      878.0  01-Jan-19  07:00\n",
      "2  11.67430   76.26332      875.0  01-Jan-19  08:00\n",
      "3  11.67464   76.26362      879.0  01-Jan-19  09:00\n",
      "4  11.67499   76.26393      888.0  01-Jan-19  10:00\n",
      "Latitude     11\n",
      "Longitude    11\n",
      "Elevation    11\n",
      "Date          0\n",
      "Time          0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seema\\AppData\\Local\\Temp\\ipykernel_16960\\1781935591.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data[[feature]] = scalers[feature].transform(test_data[[feature]])\n",
      "C:\\Users\\Seema\\AppData\\Local\\Temp\\ipykernel_16960\\1781935591.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data[[feature]] = scalers[feature].transform(test_data[[feature]])\n",
      "C:\\Users\\Seema\\AppData\\Local\\Temp\\ipykernel_16960\\1781935591.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_data[[feature]] = scalers[feature].transform(test_data[[feature]])\n",
      "e:\\Amulya B-Tech\\Code\\FINAL\\myenv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 174ms/step - loss: 0.0257 - val_loss: 6.7217e-04\n",
      "Epoch 2/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 163ms/step - loss: 0.0029 - val_loss: 7.9352e-04\n",
      "Epoch 3/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 160ms/step - loss: 0.0022 - val_loss: 6.0418e-04\n",
      "Epoch 4/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 161ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 5/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 157ms/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 6/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 155ms/step - loss: 0.0015 - val_loss: 6.8144e-04\n",
      "Epoch 7/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 167ms/step - loss: 0.0012 - val_loss: 4.6652e-04\n",
      "Epoch 8/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 160ms/step - loss: 9.8341e-04 - val_loss: 3.6875e-04\n",
      "Epoch 9/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 160ms/step - loss: 9.2259e-04 - val_loss: 2.3586e-04\n",
      "Epoch 10/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 162ms/step - loss: 8.7568e-04 - val_loss: 3.9165e-04\n",
      "Epoch 11/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 159ms/step - loss: 7.5538e-04 - val_loss: 2.3080e-04\n",
      "Epoch 12/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 159ms/step - loss: 7.3734e-04 - val_loss: 4.9697e-04\n",
      "Epoch 13/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 166ms/step - loss: 7.2426e-04 - val_loss: 1.6255e-04\n",
      "Epoch 14/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 165ms/step - loss: 6.7743e-04 - val_loss: 1.2474e-04\n",
      "Epoch 15/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 165ms/step - loss: 6.3566e-04 - val_loss: 5.6689e-04\n",
      "Epoch 16/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 176ms/step - loss: 6.3429e-04 - val_loss: 4.7281e-04\n",
      "Epoch 17/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 178ms/step - loss: 6.4891e-04 - val_loss: 1.8683e-04\n",
      "Epoch 18/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 167ms/step - loss: 5.7935e-04 - val_loss: 4.8138e-04\n",
      "Epoch 19/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 177ms/step - loss: 5.4054e-04 - val_loss: 3.6010e-04\n",
      "Epoch 20/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 176ms/step - loss: 6.7393e-04 - val_loss: 1.6415e-04\n",
      "Epoch 21/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 173ms/step - loss: 5.4716e-04 - val_loss: 3.5931e-04\n",
      "Epoch 22/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 164ms/step - loss: 5.1939e-04 - val_loss: 1.9021e-04\n",
      "Epoch 23/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 159ms/step - loss: 5.9387e-04 - val_loss: 1.4288e-04\n",
      "Epoch 24/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 162ms/step - loss: 6.8498e-04 - val_loss: 1.4217e-04\n",
      "Epoch 25/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 159ms/step - loss: 4.8664e-04 - val_loss: 2.5551e-04\n",
      "Epoch 26/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 167ms/step - loss: 5.9306e-04 - val_loss: 2.0036e-04\n",
      "Epoch 27/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 168ms/step - loss: 4.9518e-04 - val_loss: 4.1587e-04\n",
      "Epoch 28/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 184ms/step - loss: 5.7899e-04 - val_loss: 5.3771e-04\n",
      "Epoch 29/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 173ms/step - loss: 4.9036e-04 - val_loss: 9.0534e-04\n",
      "Epoch 30/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 166ms/step - loss: 5.0455e-04 - val_loss: 5.6424e-04\n",
      "Epoch 31/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 176ms/step - loss: 5.4771e-04 - val_loss: 0.0017\n",
      "Epoch 32/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 160ms/step - loss: 5.3953e-04 - val_loss: 1.5430e-04\n",
      "Epoch 33/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 152ms/step - loss: 4.2788e-04 - val_loss: 3.2050e-04\n",
      "Epoch 34/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 157ms/step - loss: 5.2012e-04 - val_loss: 4.3549e-04\n",
      "Epoch 35/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 151ms/step - loss: 5.2372e-04 - val_loss: 0.0012\n",
      "Epoch 36/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 159ms/step - loss: 5.1455e-04 - val_loss: 7.9494e-04\n",
      "Epoch 37/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 152ms/step - loss: 4.6079e-04 - val_loss: 1.4949e-04\n",
      "Epoch 38/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 152ms/step - loss: 4.7383e-04 - val_loss: 1.7973e-04\n",
      "Epoch 39/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 153ms/step - loss: 4.1410e-04 - val_loss: 2.5516e-04\n",
      "Epoch 40/40\n",
      "\u001b[1m191/191\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 149ms/step - loss: 4.0774e-04 - val_loss: 3.7960e-04\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 527ms/step\n"
     ]
    }
   ],
   "source": [
    "#Authors Amulya J N  ojaswini2004@gmail.com , Nikitha M N  nikitha.2723@gmail.com\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Bidirectional, GRU\n",
    "from keras.optimizers import Adam\n",
    "import folium\n",
    "from folium.plugins import AntPath\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "from tkinter import simpledialog\n",
    "import webbrowser\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r'E:\\Amulya B-Tech\\Code\\FINAL\\a.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values if any\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Convert 'Date' to datetime\n",
    "data['Date'] = pd.to_datetime(data['Date'], format='%d-%b-%y')\n",
    "\n",
    "# Convert 'Time' to seconds since midnight\n",
    "data['Time'] = pd.to_timedelta(data['Time'] + ':00').dt.total_seconds()\n",
    "\n",
    "# Function to prepare data\n",
    "def prepare_data(train_start_date, train_end_date, test_start_date, test_end_date):\n",
    "    # Filter data for specific training dates\n",
    "    train_data = data[(data['Date'] >= train_start_date) & (data['Date'] <= train_end_date)]\n",
    "\n",
    "    # Filter data for specific testing dates\n",
    "    test_data = data[(data['Date'] >= test_start_date) & (data['Date'] <= test_end_date)]\n",
    "\n",
    "    # Normalize 'Latitude', 'Longitude', and 'Time' separately for training data\n",
    "    scalers = {\n",
    "        'Latitude': MinMaxScaler(),\n",
    "        'Longitude': MinMaxScaler(),\n",
    "        'Time': MinMaxScaler()\n",
    "    }\n",
    "\n",
    "    for feature in scalers.keys():\n",
    "        train_data[[feature]] = scalers[feature].fit_transform(train_data[[feature]])\n",
    "\n",
    "    # Normalize 'Latitude', 'Longitude', and 'Time' for testing data using training scalers\n",
    "    for feature in scalers.keys():\n",
    "        test_data[[feature]] = scalers[feature].transform(test_data[[feature]])\n",
    "\n",
    "    # Select features and target\n",
    "    features = ['Latitude', 'Longitude', 'Time']\n",
    "    target = ['Latitude', 'Longitude']\n",
    "\n",
    "    # Prepare sequences for LSTM\n",
    "    def create_sequences(data, seq_length):\n",
    "        X = []\n",
    "        y = []\n",
    "        for i in range(seq_length, len(data)):\n",
    "            X.append(data[i-seq_length:i])\n",
    "            y.append(data[i, 0:2])  # Predicting latitude and longitude\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    seq_length = 20  # Increase sequence length\n",
    "    X_train, y_train = create_sequences(train_data[features].values, seq_length)\n",
    "    X_test, y_test = create_sequences(test_data[features].values, seq_length)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scalers\n",
    "\n",
    "# Function to build and train the model\n",
    "def train_model(X_train, y_train):\n",
    "    # Build the model\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(150, return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Bidirectional(LSTM(150, return_sequences=True)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Bidirectional(GRU(100, return_sequences=False)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(2))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=40, batch_size=32, validation_split=0.2)\n",
    "    return model\n",
    "\n",
    "# Function to make predictions, save the map, and save the predictions to a CSV file\n",
    "def predict_and_save_map(model, X_test, y_test, scalers):\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Rescale predictions\n",
    "    y_test_rescaled = np.column_stack([\n",
    "        scalers['Latitude'].inverse_transform(y_test[:, [0]]),\n",
    "        scalers['Longitude'].inverse_transform(y_test[:, [1]]),\n",
    "        scalers['Time'].inverse_transform(X_test[:, :, 2])[:, 0]  # Rescale time for color mapping\n",
    "    ])\n",
    "\n",
    "    y_pred_rescaled = np.column_stack([\n",
    "        scalers['Latitude'].inverse_transform(y_pred[:, [0]]),\n",
    "        scalers['Longitude'].inverse_transform(y_pred[:, [1]]),\n",
    "        scalers['Time'].inverse_transform(X_test[:, :, 2])[:, 0]  # Rescale time for color mapping\n",
    "    ])\n",
    "\n",
    "    # Save predictions to CSV\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'Actual Latitude': y_test_rescaled[:, 0],\n",
    "        'Actual Longitude': y_test_rescaled[:, 1],\n",
    "        'Predicted Latitude': y_pred_rescaled[:, 0],\n",
    "        'Predicted Longitude': y_pred_rescaled[:, 1]\n",
    "    })\n",
    "    predictions_df.to_csv('predictions.csv', index=False)\n",
    "\n",
    "    # Center the map around the mean latitude and longitude\n",
    "    center_lat = np.mean(y_test_rescaled[:, 0])\n",
    "    center_lng = np.mean(y_test_rescaled[:, 1])\n",
    "\n",
    "    # Create a folium map\n",
    "    m = folium.Map(location=[center_lat, center_lng], zoom_start=14)\n",
    "\n",
    "    # Add actual trajectory using AntPath\n",
    "    actual_coords = list(zip(y_test_rescaled[:, 0], y_test_rescaled[:, 1]))\n",
    "    AntPath(actual_coords, color='blue', weight=2.5, opacity=0.8).add_to(m)\n",
    "\n",
    "    # Add predicted trajectory using AntPath\n",
    "    pred_coords = list(zip(y_pred_rescaled[:, 0], y_pred_rescaled[:, 1]))\n",
    "    AntPath(pred_coords, color='red', weight=2.5, opacity=0.8).add_to(m)\n",
    "\n",
    "    # Add markers at the start points\n",
    "    folium.Marker(location=actual_coords[0], popup='Actual Start', icon=folium.Icon(color='blue')).add_to(m)\n",
    "    folium.Marker(location=pred_coords[0], popup='Predicted Start', icon=folium.Icon(color='red')).add_to(m)\n",
    "\n",
    "    # Save the map to an HTML file and open it\n",
    "    map_file = 'trajectory_map.html'\n",
    "    m.save(map_file)\n",
    "    webbrowser.open(map_file)\n",
    "\n",
    "# Function to handle prediction process\n",
    "def on_predict():\n",
    "    try:\n",
    "        train_start_date = start_date_entry.get()\n",
    "        train_end_date = end_date_entry.get()\n",
    "        test_start_date = test_start_date_entry.get()\n",
    "        test_end_date = test_end_date_entry.get()\n",
    "        \n",
    "        X_train, y_train, X_test, y_test, scalers = prepare_data(train_start_date, train_end_date, test_start_date, test_end_date)\n",
    "        model = train_model(X_train, y_train)\n",
    "        predict_and_save_map(model, X_test, y_test, scalers)\n",
    "        messagebox.showinfo(\"Success\", \"Prediction completed, map saved, and CSV file created!\")\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Error\", str(e))\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Trajectory Prediction\")\n",
    "\n",
    "# Create and place labels and entries\n",
    "tk.Label(root, text=\"Training Start Date (YYYY-MM-DD):\").grid(row=0, column=0)\n",
    "start_date_entry = tk.Entry(root)\n",
    "start_date_entry.grid(row=0, column=1)\n",
    "\n",
    "tk.Label(root, text=\"Training End Date (YYYY-MM-DD):\").grid(row=1, column=0)\n",
    "end_date_entry = tk.Entry(root)\n",
    "end_date_entry.grid(row=1, column=1)\n",
    "\n",
    "tk.Label(root, text=\"Testing Start Date (YYYY-MM-DD):\").grid(row=2, column=0)\n",
    "test_start_date_entry = tk.Entry(root)\n",
    "test_start_date_entry.grid(row=2, column=1)\n",
    "\n",
    "tk.Label(root, text=\"Testing End Date (YYYY-MM-DD):\").grid(row=3, column=0)\n",
    "test_end_date_entry = tk.Entry(root)\n",
    "test_end_date_entry.grid(row=3, column=1)\n",
    "\n",
    "# Create and place the predict button\n",
    "predict_button = tk.Button(root, text=\"Predict\", command=on_predict)\n",
    "predict_button.grid(row=4, column=0, columnspan=2)\n",
    "\n",
    "# Run the GUI loop\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
